{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import and install of spark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "5\n",
      "defaultdict(<class 'int'>, {'scala': 1, 'java': 1, 'spark': 1, 'hadoop': 1, 'akka': 1})\n"
     ]
    }
   ],
   "source": [
    "#### This cell is to make spark work on a windows laptop\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\spark-2.0.1-bin-hadoop2.7\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\")\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\")\n",
    "#os.environ['SPARK_EXECUTOR_MEMORY']=\"5G\"\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SQLContext\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext('local')\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\"])\n",
    "print (words.count())\n",
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import data and take off header</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before cleaning header: 188319\n",
      "number of rows without header: 188318\n",
      "Number of partitions :2\n"
     ]
    }
   ],
   "source": [
    "input_path = 'train.csv'\n",
    "raw_data = sc.textFile(input_path)\n",
    "\n",
    "print(\"number of rows before cleaning header:\",raw_data.count())\n",
    "\n",
    "header = raw_data.first()\n",
    "\n",
    "cleaned_data = raw_data.filter(lambda row : row != header)\n",
    "\n",
    "print(\"number of rows without header:\",cleaned_data.count())\n",
    "print('Number of partitions :'+str(cleaned_data.getNumPartitions()))\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'loss']\n",
      "Length of names 132\n"
     ]
    }
   ],
   "source": [
    "names = header.split(';')[0].split(',')\n",
    "print(names)\n",
    "print(\"Length of names\",len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = names[1:117]\n",
    "conts = names[117:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_StructField(string):\n",
    "    hint = string[:3]\n",
    "    if hint == \"cat\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"con\":\n",
    "        datatype = types.FloatType()\n",
    "    elif hint == \"id\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"los\":\n",
    "        datatype = types.FloatType()\n",
    "    else:\n",
    "        raise ValueError(\"Can\\'t read this string:\" + hint )\n",
    "\n",
    "    return types.StructField(string, datatype, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list = [create_StructField(string) for string in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_schema = types.StructType(structField_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryeval(val,column_number):\n",
    "    if column_number == 0:\n",
    "        return int(val)\n",
    "    elif 1 <= column_number <= 116:\n",
    "        return val\n",
    "    elif 117 <= column_number <= 131:\n",
    "        return float(val)\n",
    "    else:\n",
    "        raise Exception(\"There is a big problem\")\n",
    "\n",
    "def to_tuple(row):\n",
    "    list_strings = row.split(',')\n",
    "    return tuple(tryeval(val, n) for n, val in enumerate(list_strings))\n",
    "\n",
    "cleaned_data_splitted = cleaned_data.map(lambda x:to_tuple(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tuples(list_):\n",
    "    return tuple((string,) for string in list_)\n",
    "\n",
    "def fusion(x, y):\n",
    "    return tuple(tuple(set(xi + yi)) for xi, yi in zip(x,y))\n",
    "\n",
    "list_of_dictionaries = []\n",
    "a = cleaned_data_splitted.map(lambda x: to_tuples(x[1:117]))\n",
    "a = a.reduce(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tuples = tuple(tuple(sorted(tup)) for tup in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tup in sorted_tuples:\n",
    "    my_dict = dict()\n",
    "    for idx, cat in enumerate(tup):\n",
    "        my_dict[cat] = idx\n",
    "    list_of_dictionaries.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bListOfDictionaries = sc.broadcast(list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(row):\n",
    "    strings = row[1:117]\n",
    "    my_dicts = bListOfDictionaries.value\n",
    "    tuple_of_ints = ()\n",
    "    for dict_, string in zip(my_dicts, strings):\n",
    "        try:\n",
    "            tuple_of_ints += (dict_[string],)\n",
    "        except KeyError:\n",
    "            tuple_of_ints += (0,)\n",
    "    return (row[0],) + tuple_of_ints + row[117:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_rdd = cleaned_data_splitted.map(lambda row:replace(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd, schema = data_schema).coalesce(12).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_frequencies(column_idx, max_cat = 10):\n",
    "    name = names[column_idx]\n",
    "    a = df.groupBy(name).count().collect()\n",
    "    pdf = pd.DataFrame(data=a).sort_values(0)\n",
    "    l1 = pdf[0].tolist()[:10]\n",
    "    l2 = pdf[1].tolist()[:10]\n",
    "    plt.bar(range(len(l1)), l2)\n",
    "    plt.xticks(np.arange(len(l1))+0.5,l1)\n",
    "    plt.title(\"Frequencies of \" + name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import KernelDensity\n",
    "\n",
    "def plotDistribution(rdd, plot=True, numSamples=1000):\n",
    "    rdd.cache()\n",
    "    vmin = rdd.min()\n",
    "    vmax = rdd.max()\n",
    "    \n",
    "    if vmin==vmax:\n",
    "        return None, None\n",
    "    \n",
    "    stddev = rdd.stdev()\n",
    "    \n",
    "    domain = np.arange(vmin, vmax, (vmax-vmin)/numSamples)\n",
    "    \n",
    "    # a simple heuristic to select bandwidth\n",
    "    bandwidth = 1.06 * stddev * pow(rdd.count(), -.2)\n",
    "    \n",
    "    \n",
    "    kd = KernelDensity()\n",
    "    kd.setSample(rdd)\n",
    "    kd.setBandwidth(bandwidth)\n",
    "    density = kd.estimate(domain)\n",
    "    \n",
    "    rdd.unpersist()\n",
    "    \n",
    "    # plot\n",
    "    if(plot):\n",
    "        plt.plot(domain, density)\n",
    "        plt.xlim(0,20000)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return domain,density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def densityEstimation(cat_number,max_cat=5):\n",
    "    \n",
    "    cat_loss = df.select([cats[cat_number-1],\"loss\"]).rdd.cache()\n",
    "    \n",
    "    for integer in range(len(sorted_tuples[cat_number-1])):\n",
    "        \n",
    "        #Selecting the losses belonging to this category\n",
    "        my_rdd = cat_loss.filter(lambda x: x[0] == integer)\n",
    "        \n",
    "        samples = my_rdd.map(lambda x: x[1])\n",
    "        domain, density = plotDistribution(samples, False)\n",
    "        \n",
    "        # This is not done if the category has only one entry.\n",
    "        if domain is not None and density is not None:\n",
    "        \n",
    "            plt.plot(domain,density, label=\"category: \" + str(integer))\n",
    "\n",
    "            if integer%max_cat == max_cat-1:\n",
    "                plt.xlim(0,20000)\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.title(\"Categorical feature N° \" + str(cat_number))\n",
    "                plt.show()\n",
    "            \n",
    "    if integer%max_cat !=max_cat-1:\n",
    "        plt.xlim(0,20000)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Categorical feature N° \" + str(cat_number))\n",
    "        plt.show()\n",
    "    cat_loss.unpersist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_and_plot(cont_idx):\n",
    "    symplified_rdd = df.select([conts[cont_idx],\"loss\"]).rdd\n",
    "    sample = symplified_rdd.takeSample(False, 2000)\n",
    "    continuous_sample = []\n",
    "    loss_sample = []\n",
    "    for row in sample:\n",
    "        continuous_sample.append(row[0])\n",
    "        loss_sample.append(row[1])\n",
    "    plt.scatter(continuous_sample, loss_sample, s = 0.07)\n",
    "    plt.title(\"Continuous feature N°\" + str(cont_idx+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>PART OF MAKING A PREDICTION WITH MultilayerPerceptronClassifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_index(tup):\n",
    "    result = ()\n",
    "    for idx in list_indices:\n",
    "        result += (tup[idx-1],)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "list_indices = [i for i in range(2,last)]\n",
    "last = len(names)\n",
    "L = df.rdd.count()\n",
    "print(last)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list_selected = [create_StructField(string) for i,string in enumerate(names) if i in list_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df.rdd.map(lambda x: (x[-1])).mean()\n",
    "stdev = df.rdd.map(lambda x: (x[-1])).stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2904.07847613\n"
     ]
    }
   ],
   "source": [
    "print(stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Perceptron = sqlContext.createDataFrame(df.rdd.map(lambda x: (float(x[-1]-mean), Vectors.dense(keep_index(x)))), [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               label|\n",
      "+--------------------+\n",
      "|-0.28379321034954846|\n",
      "| -0.6038878509372658|\n",
      "|-0.01110424455208...|\n",
      "| -0.7222558643451996|\n",
      "|-0.09417362171162356|\n",
      "|  0.7250260104909019|\n",
      "| -0.6560145431486457|\n",
      "| 0.18884211257377148|\n",
      "|  2.4940312630537584|\n",
      "|   1.083735231032165|\n",
      "|  1.1568256298113546|\n",
      "|  1.0083723009046492|\n",
      "| -0.6350681125941565|\n",
      "| -0.6768300795292171|\n",
      "| -0.8443841008324754|\n",
      "| -0.5653730599268415|\n",
      "|  1.2299881590987147|\n",
      "|-0.13038137138117617|\n",
      "| 0.38910179184875254|\n",
      "|  0.2618910657346301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Perceptron.select(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0, 0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Perceptron.select(\"features\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Perceptron_train.select(\"label\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32).fit(df_Perceptron)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(df_Perceptron_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(100)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>PART OF MAKING A PREDICTION WITH ?</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1.35335746922e-05,-1051.51414254,1227.4934644,1698.16241385,990.503116633,1047.74607516,364.047070749,4416.08406751,526.944026828]\n",
      "Intercept: 1747.4076279010137\n",
      "Coefficient Standard Errors: [3.5212580562094656e-05, 14.106974375468429, 14.000582331838448, 26.783596031267724, 13.773924484722937, 13.671555523887191, 15.292001256525488, 39.08956700536803, 25.729132029377983, 18.654263194171712]\n",
      "T Values: [0.38433919003359335, -74.53860158447762, 87.67445776954186, 63.40307746073159, 71.91146704279849, 76.63693230312296, 23.80637201386897, 112.97347107749498, 20.48044318891395, 93.67336622799282]\n",
      "P Values: [0.7007274989021783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dispersion: 6695589.685371516\n",
      "Null Deviance: 1588212205186.7375\n",
      "Residual Degree Of Freedom Null: 188317\n",
      "Deviance: 1260833102472.9395\n",
      "Residual Degree Of Freedom: 188308\n",
      "AIC: 3494221.733282857\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-1752.2242908288758|\n",
      "|-1691.3011437839896|\n",
      "|-1017.5571472425559|\n",
      "|-1974.0402261502595|\n",
      "|-1201.5542601489979|\n",
      "| 2167.9688489482514|\n",
      "| -1662.933921830464|\n",
      "|-379.65447960741994|\n",
      "| 3568.8871860944164|\n",
      "| 1797.8952807295027|\n",
      "|  3421.948667014105|\n",
      "|  1943.082366395478|\n",
      "| -550.5899718414817|\n",
      "|-1035.9171666227767|\n",
      "| -2209.974346465772|\n",
      "|-1763.7514722526512|\n",
      "| 1936.2556549234014|\n",
      "|  547.2445623091544|\n",
      "|  357.7020079750441|\n",
      "| 1059.9784036973547|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(df_Perceptron)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Load training data\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(df_Perceptron_train)\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
