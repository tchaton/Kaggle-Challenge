{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import and install of spark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "5\n",
      "defaultdict(<class 'int'>, {'scala': 1, 'java': 1, 'spark': 1, 'hadoop': 1, 'akka': 1})\n"
     ]
    }
   ],
   "source": [
    "#### This cell is to make spark work on a windows laptop\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\spark-2.0.1-bin-hadoop2.7\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\")\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\")\n",
    "#os.environ['SPARK_EXECUTOR_MEMORY']=\"5G\"\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SQLContext\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext('local')\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\"])\n",
    "print (words.count())\n",
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import data and take off header</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before cleaning header: 188319\n",
      "number of rows without header: 188318\n",
      "Number of partitions :2\n"
     ]
    }
   ],
   "source": [
    "input_path = 'train.csv'\n",
    "raw_data = sc.textFile(input_path)\n",
    "\n",
    "print(\"number of rows before cleaning header:\",raw_data.count())\n",
    "\n",
    "header = raw_data.first()\n",
    "\n",
    "cleaned_data = raw_data.filter(lambda row : row != header)\n",
    "\n",
    "print(\"number of rows without header:\",cleaned_data.count())\n",
    "print('Number of partitions :'+str(cleaned_data.getNumPartitions()))\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'loss']\n",
      "Length of names 132\n"
     ]
    }
   ],
   "source": [
    "names = header.split(';')[0].split(',')\n",
    "print(names)\n",
    "print(\"Length of names\",len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = names[1:117]\n",
    "conts = names[117:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_StructField(string):\n",
    "    hint = string[:3]\n",
    "    if hint == \"cat\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"con\":\n",
    "        datatype = types.FloatType()\n",
    "    elif hint == \"id\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"los\":\n",
    "        datatype = types.FloatType()\n",
    "    else:\n",
    "        raise ValueError(\"Can\\'t read this string:\" + hint )\n",
    "\n",
    "    return types.StructField(string, datatype, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list = [create_StructField(string) for string in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_schema = types.StructType(structField_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryeval(val,column_number):\n",
    "    if column_number == 0:\n",
    "        return int(val)\n",
    "    elif 1 <= column_number <= 116:\n",
    "        return val\n",
    "    elif 117 <= column_number <= 131:\n",
    "        return float(val)\n",
    "    else:\n",
    "        raise Exception(\"There is a big problem\")\n",
    "\n",
    "def to_tuple(row):\n",
    "    list_strings = row.split(',')\n",
    "    return tuple(tryeval(val, n) for n, val in enumerate(list_strings))\n",
    "\n",
    "cleaned_data_splitted = cleaned_data.map(lambda x:to_tuple(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tuples(list_):\n",
    "    return tuple((string,) for string in list_)\n",
    "\n",
    "def fusion(x, y):\n",
    "    return tuple(tuple(set(xi + yi)) for xi, yi in zip(x,y))\n",
    "\n",
    "list_of_dictionaries = []\n",
    "a = cleaned_data_splitted.map(lambda x: to_tuples(x[1:117]))\n",
    "a = a.reduce(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tuples = tuple(tuple(sorted(tup)) for tup in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tup in sorted_tuples:\n",
    "    my_dict = dict()\n",
    "    for idx, cat in enumerate(tup):\n",
    "        my_dict[cat] = idx\n",
    "    list_of_dictionaries.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bListOfDictionaries = sc.broadcast(list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(row):\n",
    "    strings = row[1:117]\n",
    "    my_dicts = bListOfDictionaries.value\n",
    "    tuple_of_ints = ()\n",
    "    for dict_, string in zip(my_dicts, strings):\n",
    "        try:\n",
    "            tuple_of_ints += (dict_[string],)\n",
    "        except KeyError:\n",
    "            tuple_of_ints += (0,)\n",
    "    return (row[0],) + tuple_of_ints + row[117:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_rdd = cleaned_data_splitted.map(lambda row:replace(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd, schema = data_schema).coalesce(12).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_frequencies(column_idx, max_cat = 10):\n",
    "    name = names[column_idx]\n",
    "    a = df.groupBy(name).count().collect()\n",
    "    pdf = pd.DataFrame(data=a).sort_values(0)\n",
    "    l1 = pdf[0].tolist()[:10]\n",
    "    l2 = pdf[1].tolist()[:10]\n",
    "    plt.bar(range(len(l1)), l2)\n",
    "    plt.xticks(np.arange(len(l1))+0.5,l1)\n",
    "    plt.title(\"Frequencies of \" + name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import KernelDensity\n",
    "\n",
    "def plotDistribution(rdd, plot=True, numSamples=1000):\n",
    "    rdd.cache()\n",
    "    vmin = rdd.min()\n",
    "    vmax = rdd.max()\n",
    "    \n",
    "    if vmin==vmax:\n",
    "        return None, None\n",
    "    \n",
    "    stddev = rdd.stdev()\n",
    "    \n",
    "    domain = np.arange(vmin, vmax, (vmax-vmin)/numSamples)\n",
    "    \n",
    "    # a simple heuristic to select bandwidth\n",
    "    bandwidth = 1.06 * stddev * pow(rdd.count(), -.2)\n",
    "    \n",
    "    \n",
    "    kd = KernelDensity()\n",
    "    kd.setSample(rdd)\n",
    "    kd.setBandwidth(bandwidth)\n",
    "    density = kd.estimate(domain)\n",
    "    \n",
    "    rdd.unpersist()\n",
    "    \n",
    "    # plot\n",
    "    if(plot):\n",
    "        plt.plot(domain, density)\n",
    "        plt.xlim(0,20000)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return domain,density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def densityEstimation(cat_number,max_cat=5):\n",
    "    \n",
    "    cat_loss = df.select([cats[cat_number-1],\"loss\"]).rdd.cache()\n",
    "    \n",
    "    for integer in range(len(sorted_tuples[cat_number-1])):\n",
    "        \n",
    "        #Selecting the losses belonging to this category\n",
    "        my_rdd = cat_loss.filter(lambda x: x[0] == integer)\n",
    "        \n",
    "        samples = my_rdd.map(lambda x: x[1])\n",
    "        domain, density = plotDistribution(samples, False)\n",
    "        \n",
    "        # This is not done if the category has only one entry.\n",
    "        if domain is not None and density is not None:\n",
    "        \n",
    "            plt.plot(domain,density, label=\"category: \" + str(integer))\n",
    "\n",
    "            if integer%max_cat == max_cat-1:\n",
    "                plt.xlim(0,20000)\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.title(\"Categorical feature N° \" + str(cat_number))\n",
    "                plt.show()\n",
    "            \n",
    "    if integer%max_cat !=max_cat-1:\n",
    "        plt.xlim(0,20000)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Categorical feature N° \" + str(cat_number))\n",
    "        plt.show()\n",
    "    cat_loss.unpersist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_and_plot(cont_idx):\n",
    "    symplified_rdd = df.select([conts[cont_idx],\"loss\"]).rdd\n",
    "    sample = symplified_rdd.takeSample(False, 2000)\n",
    "    continuous_sample = []\n",
    "    loss_sample = []\n",
    "    for row in sample:\n",
    "        continuous_sample.append(row[0])\n",
    "        loss_sample.append(row[1])\n",
    "    plt.scatter(continuous_sample, loss_sample, s = 0.07)\n",
    "    plt.title(\"Continuous feature N°\" + str(cont_idx+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>PART OF MAKING A PREDICTION WITH MultilayerPerceptronClassifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_index(tup):\n",
    "    result = ()\n",
    "    for idx in list_indices:\n",
    "        result += (tup[idx-1],)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd, schema = data_schema).coalesce(12).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "list_indices = [i for i in range(2,15)]\n",
    "last = len(names)\n",
    "L = df.rdd.count()\n",
    "print(last)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list_selected = [create_StructField(string) for i,string in enumerate(names) if i in list_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df.rdd.map(lambda x: (x[-1])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Perceptron = sqlContext.createDataFrame(df.rdd.map(lambda x: (float(x[-1]-mean), Vectors.dense(keep_index(x)))), [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               label|\n",
      "+--------------------+\n",
      "|-0.28379321034954846|\n",
      "| -0.6038878509372658|\n",
      "|-0.01110424455208...|\n",
      "| -0.7222558643451996|\n",
      "|-0.09417362171162356|\n",
      "|  0.7250260104909019|\n",
      "| -0.6560145431486457|\n",
      "| 0.18884211257377148|\n",
      "|  2.4940312630537584|\n",
      "|   1.083735231032165|\n",
      "|  1.1568256298113546|\n",
      "|  1.0083723009046492|\n",
      "| -0.6350681125941565|\n",
      "| -0.6768300795292171|\n",
      "| -0.8443841008324754|\n",
      "| -0.5653730599268415|\n",
      "|  1.2299881590987147|\n",
      "|-0.13038137138117617|\n",
      "| 0.38910179184875254|\n",
      "|  0.2618910657346301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Perceptron.select(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0, 0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Perceptron.select(\"features\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Perceptron_train.select(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1783.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1783.0 (TID 25324, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-442-73ddf283b20e>\", line 3, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-442-73ddf283b20e>\", line 3, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-442-73ddf283b20e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_Perceptron_tuple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_Perceptron_tuple_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_Perceptron_tuple_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_Perceptron_tuple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \"\"\"\n\u001b[1;32m-> 1328\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1783.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1783.0 (TID 25324, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-442-73ddf283b20e>\", line 3, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-442-73ddf283b20e>\", line 3, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "list_indices = [i for i in range(2,20)]\n",
    "df_Perceptron_tuple = sqlContext.createDataFrame(df.rdd.map(lambda x: (float(x[-1]-mean), keep_index(x))), [\"label\", \"features\"])\n",
    "(df_Perceptron_tuple_train,df_Perceptron_tuple_test) = df_Perceptron_tuple.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "layers = [len(list_indices), 5, 4, 5]\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "# train the model\n",
    "model = trainer.fit(df_Perceptron_tuple_train)\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(df_Perceptron_tuple_test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|         prediction|              label|            features|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|-1503.6209997072276|-3027.3376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "|  -776.632074824496| -3017.227684877636|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-3001.3376854879875|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2990.0576867086907|[1.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2982.9976853353996|[0.0,0.0,0.0,0.0,...|\n",
      "| 232.29923653909066|  -2980.04768457246|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2960.9876870138664|[1.0,0.0,0.0,0.0,...|\n",
      "| 232.29923653909066|-2957.3376854879875|[0.0,0.0,0.0,1.0,...|\n",
      "| 3358.4996599156466|-2957.3376854879875|[0.0,1.0,0.0,1.0,...|\n",
      "|  772.0847292517028|-2957.3376854879875|[0.0,1.0,0.0,1.0,...|\n",
      "|-1503.6209997072276| -2949.757683656933|[1.0,0.0,0.0,1.0,...|\n",
      "| -865.9238621219307|-2941.3376854879875|[0.0,1.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2925.3376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2922.3376854879875|[1.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2918.2676857931633|[0.0,0.0,0.0,0.0,...|\n",
      "|  -579.645135718125|-2915.1376885397453|[0.0,1.0,0.0,1.0,...|\n",
      "|   687.464778083675| -2914.227684877636|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2912.067688844921|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2910.9876870138664|[1.0,0.0,0.0,1.0,...|\n",
      "|   687.464778083675|-2907.3376854879875|[0.0,0.0,1.0,0.0,...|\n",
      "| -865.9238621219307|-2898.8876885397453|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|   -2887.9676903708|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2883.507683656933|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2879.917687319042|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2877.3376854879875|[1.0,0.0,0.0,1.0,...|\n",
      "| -865.9238621219307| -2872.987679384472|[1.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2864.207680605175|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2862.3376854879875|[0.0,1.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2862.1376885397453|[1.0,0.0,0.0,1.0,...|\n",
      "|   687.464778083675|-2857.3376854879875|[0.0,0.0,1.0,0.0,...|\n",
      "|-1503.6209997072276|-2855.0976799948235|[1.0,1.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2852.6576928122063|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2851.957680605175|[1.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276| -2850.187691591503|[0.0,0.0,0.0,1.0,...|\n",
      "| -865.9238621219307|-2845.5476922018547|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2844.777687929394|[1.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2843.1176842672844|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2842.1576928122063|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276| -2839.477684877636|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|   -2839.4676903708|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2837.257683656933|[1.0,1.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2836.987679384472|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2835.737679384472|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2833.8576897604485|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2823.7876824362297|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2821.8176812155266|[0.0,0.0,0.0,1.0,...|\n",
      "| -865.9238621219307|-2808.8176812155266|[0.0,1.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2803.8376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2801.5476922018547|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2800.917687319042|[1.0,1.0,0.0,0.0,...|\n",
      "| 232.29923653909066| -2800.007683656933|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2794.8676842672844|[1.0,1.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2792.6476830465813|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2792.517678163769|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2791.0976799948235|[0.0,1.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2789.6576928122063|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2788.5876854879875|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2785.267678163769|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2781.247689150097|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276| -2779.037697695019|[1.0,0.0,0.0,1.0,...|\n",
      "|   687.464778083675|-2775.3276757223625|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2773.3976830465813|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2773.2976769430657|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|  -2773.19767083955|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2773.1276940329094|[1.0,1.0,0.0,0.0,...|\n",
      "|   687.464778083675|-2770.7976769430657|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2770.3776940329094|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2768.787697695019|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2768.417672060253|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2765.437691591503|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|  -2764.69767083955|[1.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2763.037697695019|[1.0,0.0,0.0,1.0,...|\n",
      "|   687.464778083675|-2762.8776940329094|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2762.787697695019|[0.0,0.0,0.0,0.0,...|\n",
      "|   687.464778083675|-2761.0676964743157|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2759.4076928122063|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2759.1376732809563|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2758.8476952536125|[0.0,1.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2757.937691591503|[0.0,1.0,0.0,0.0,...|\n",
      "|   687.464778083675|-2757.3376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "|  1088.682629345709|-2757.3376854879875|[0.0,1.0,1.0,0.0,...|\n",
      "|-1503.6209997072276| -2755.167672060253|[1.0,0.0,0.0,0.0,...|\n",
      "|   8367.25445660781|   -2753.9676903708|[0.0,1.0,1.0,1.0,...|\n",
      "|-1503.6209997072276|-2752.8276757223625|[0.0,0.0,1.0,0.0,...|\n",
      "|-1503.6209997072276| -2752.707680605175|[1.0,1.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2752.237679384472|[1.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2751.6576928122063|[1.0,1.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2751.517678163769|[0.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276| -2750.737679384472|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2750.3976830465813|[0.0,0.0,0.0,0.0,...|\n",
      "|   687.464778083675| -2745.927681825878|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2745.8376854879875|[1.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307| -2745.537697695019|[0.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2744.5976952536125|[1.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2743.8376854879875|[1.0,0.0,0.0,1.0,...|\n",
      "| -865.9238621219307| -2743.517678163769|[0.0,0.0,0.0,0.0,...|\n",
      "| -865.9238621219307|-2742.1476830465813|[1.0,0.0,0.0,0.0,...|\n",
      "|-1503.6209997072276|-2741.8976830465813|[1.0,1.0,0.0,0.0,...|\n",
      "|-1503.6209997072276| -2741.777687929394|[1.0,0.0,0.0,1.0,...|\n",
      "|-1503.6209997072276|-2741.1576928122063|[1.0,0.0,0.0,0.0,...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 1425.93\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_43ce9fd403e544073d42) of depth 5 with 59 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32).fit(df_Perceptron)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(df_Perceptron_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(100)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>PART OF MAKING A PREDICTION WITH ?</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [1.35335746922e-05,-1051.51414254,1227.4934644,1698.16241385,990.503116633,1047.74607516,364.047070749,4416.08406751,526.944026828]\n",
      "Intercept: 1747.4076279010137\n",
      "Coefficient Standard Errors: [3.5212580562094656e-05, 14.106974375468429, 14.000582331838448, 26.783596031267724, 13.773924484722937, 13.671555523887191, 15.292001256525488, 39.08956700536803, 25.729132029377983, 18.654263194171712]\n",
      "T Values: [0.38433919003359335, -74.53860158447762, 87.67445776954186, 63.40307746073159, 71.91146704279849, 76.63693230312296, 23.80637201386897, 112.97347107749498, 20.48044318891395, 93.67336622799282]\n",
      "P Values: [0.7007274989021783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dispersion: 6695589.685371516\n",
      "Null Deviance: 1588212205186.7375\n",
      "Residual Degree Of Freedom Null: 188317\n",
      "Deviance: 1260833102472.9395\n",
      "Residual Degree Of Freedom: 188308\n",
      "AIC: 3494221.733282857\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-1752.2242908288758|\n",
      "|-1691.3011437839896|\n",
      "|-1017.5571472425559|\n",
      "|-1974.0402261502595|\n",
      "|-1201.5542601489979|\n",
      "| 2167.9688489482514|\n",
      "| -1662.933921830464|\n",
      "|-379.65447960741994|\n",
      "| 3568.8871860944164|\n",
      "| 1797.8952807295027|\n",
      "|  3421.948667014105|\n",
      "|  1943.082366395478|\n",
      "| -550.5899718414817|\n",
      "|-1035.9171666227767|\n",
      "| -2209.974346465772|\n",
      "|-1763.7514722526512|\n",
      "| 1936.2556549234014|\n",
      "|  547.2445623091544|\n",
      "|  357.7020079750441|\n",
      "| 1059.9784036973547|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(df_Perceptron)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-404.315220427,345.383560765,325.491764574,44.6593857513,116.793006981,-283.871676436,772.400810041,82.099727242,85.3049925541,231.181604729,155.313626528,752.396821242,120.567192696,22.4687480077,-1113.91265408,-1250.57048298,-813.760631587,-1168.8405701,-719.68592591,-1119.17611066,-1555.96795226,-2275.25514529,430.722922743,142.027474825,588.454622172,835.98414292,617.732820159,263.053647378,595.778222254,150.788859702,197.441419838,871.071305974,229.47404198,497.614473905,966.581281526,-141.784594484,-10.9635229793,156.687718875,-321.393257369,-242.879333557,-460.723619908,318.125102236,-317.442511445,330.040172214,-238.272572953,-130.823275361,79.5774827231,-36.0796321435,470.822546968,339.22617848,547.159400446,470.354368377,870.065211336,117.037212979,456.984089387,424.599744178,2937.09265927,-582.83864822,-386.617630202,-1123.93657734,-1257.08988231,-348.49317364,-1237.22544744,-106.422317647,287.3264544,-184.60898337,543.803864888,353.351647526,-314.09251434,679.964541573,237.05510214,386.608497174,-299.942052534,53.3564468181,-145.463063811,294.942954169,551.12478203,405.332359804,654.440326723,-567.786576431,-369.662351619,-180.677900146,-39.3883471495,-21.1671989445,313.045055325,133.485877141,372.487474986,166.534501326,377.400823738,840.184784667,8.68970841684,15.8482823594,23.9426085006,-22.6886201624,49.2914330499,32.9406014054,29.0972268647,-54.2761491786,-16.5310686782,14.8760949138,65.1351415479,27.432750056,410.283233277,126.096021149,180.113598521,-51.7780507781,-33.7515261592,-10.7365204888,1.15907385989,0.393250549321,-68.9195782819,-4.90309090948,-0.174709324819,34.4987874751,13.5084596909,-0.444445254882,-1952.94437648,1440.68442109,676.484795584,-420.801263705,197.7026687,90.6049685976,2381.34719398,353.365188187,722.538465517,465.566943005,-868.475372604,-702.285258338,-100.439093681,698.917223024]\n",
      "Intercept: -5140.306894903365\n",
      "Coefficient Standard Errors: [14.856529858950587, 36.272346007912844, 98.17117373122088, 30.83285912789786, 24.136359454719482, 38.237749164280594, 163.37287368824468, 94.40548945463998, 62.232641785228466, 56.812619003887136, 57.58224833973851, 57.098655987065015, 57.5990462852601, 106.88722976537751, 449.69997389244685, 1364.6815312960468, 1367.049714524251, 1366.7489799864147, 1365.5221731616423, 1372.0847103870071, 1367.4118817884537, 1394.0222235585034, 52.02857859608686, 61.87225631961999, 57.221243875406316, 56.21724426714027, 52.59327185930957, 62.06019179515172, 68.23021100468507, 68.12747233627721, 59.234184275027594, 88.50010062715678, 114.13332797369245, 126.85742918426843, 179.78840485365336, 406.8552164624033, 406.93265522427924, 407.0633860597414, 408.083441012711, 407.7813555830966, 407.94057343921395, 411.26908950704666, 408.8843856869246, 407.1523091651988, 408.79419828915377, 419.84740981662645, 421.586450104331, 433.1976390062845, 37.44609450648004, 42.74625984336076, 109.20124490174068, 37.92766402579534, 34.457072140943104, 44.21630286921294, 222.8925149762224, 200.17214528329924, 1269.9957594644034, 1290.3028741667854, 1285.0653408155188, 1276.1544441761207, 1270.6106254653503, 1303.0566801387058, 1267.299098986479, 1269.69667003567, 109.67470214998204, 114.09066580436445, 144.89675389126535, 226.41963636793943, 181.18551590998385, 506.18374211546563, 33.52237219167868, 13.550093354767759, 16.516363325159407, 41.95009648808101, 17.107960378998584, 81.55109041978623, 98.24287589252329, 56.88112952856266, 10.127923808519586, 9.687029399130546, 9.663150376198251, 8.806995076149924, 8.34871391788624, 12.46249604160224, 46.746119374191146, 31.193171930979517, 15.137413186803986, 17.325270572381452, 1316.65028677056, 1371.0482969257055, 5.658461880565119, 4.060929955459795, 13.845958835481374, 8.72589271963398, 16.009823901653363, 15.270401184453565, 4.401038723675875, 14.944406982391447, 3.4536238905666, 1.9408316550994302, 17.60393112550814, 40.805300818163055, 406.3314128346854, 11.382344060816076, 13.681232071936721, 12.244243176412928, 14.623069072131813, 3.1226458789829774, 0.48572601673126287, 0.20370808190222645, 29.763440989026293, 0.4599960310832422, 0.4284359700825336, 8.070598224652295, 15.87538969724806, 0.28677316373734596, 171.61226227163505, 43.17360478797692, 84.92638795417507, 45.611378851168915, 31.44425238919413, 210.13300481997652, 73.27242786240804, 44.40234814093122, 122.71187442837648, 162.38366582096933, 275.5509839994084, 319.2986457673829, 304.58330788001285, 27.38073466329753, 351.75954307535034]\n",
      "T Values: [-27.214647314387005, 9.521952638242526, 3.3155533564807977, 1.448434787252016, 4.8388824834866995, -7.423859474989627, 4.727840017766927, 0.8696499294298745, 1.3707435536556836, 4.0691946399034045, 2.6972483882860616, 13.177137153850529, 2.093215087250777, 0.21020984505798887, -2.4770129391805535, -0.9163826536086007, -0.5952677674710245, -0.8551976897140029, -0.527040820028654, -0.8156756665161488, -1.1378926664212128, -1.6321512719349127, 8.278583316419324, 2.2954953200910575, 10.283848835120903, 14.870599827832933, 11.745472345037573, 4.238685698015388, 8.731883039504408, 2.2133341298454567, 3.333234385754241, 9.842602435490155, 2.0105787332553158, 3.922627764923044, 5.37621590398421, -0.3484890662512369, -0.02694186086692312, 0.38492216259413736, -0.7875675047515022, -0.5956116684389083, -1.1293890578807566, 0.7735205741277251, -0.7763625184941192, 0.8106061657626145, -0.5828668164813582, -0.31159719532044805, 0.18875721148868294, -0.08328677004397657, 12.573341844414681, 7.935809582478673, 5.010560098823356, 12.401353483234967, 25.25070057540557, 2.6469244460669525, 2.0502442149549407, 2.121172971280692, 2.3126791072951622, -0.4517068510729008, -0.3008544530168077, -0.8807214381180817, -0.9893588618872875, -0.2674428357196896, -0.9762694918902588, -0.0838171195988724, 2.619806106306878, -1.6180901572282407, 3.753043807291707, 1.5606051365252382, -1.733540966353918, 1.3433156480518513, 7.071549136933364, 28.53179583723632, -18.16029634545356, 1.271902838965327, -8.502653769850754, 3.616664751518697, 5.609819307742977, 7.125954831831877, 64.61742200043382, -58.61307455950191, -38.254848287369185, -20.515272074489797, -4.717893981869023, -1.6984718690283744, 6.696706796538436, 4.2793300224800745, 24.607075884732776, 9.612230910368014, 0.2866371029041041, 0.6128046594352818, 1.5357015033858725, 3.9026239145353414, 1.7292127461223141, -2.6001488777590525, 3.0788241864942245, 2.1571536338521544, 6.611445318154232, -3.631870387532001, -4.78658626474484, 7.664804350618995, 3.700033877861489, 0.6722839804138047, 1.0097256089927706, 11.078212051542021, 13.165013031990423, -4.2287669423167165, -2.3081013973669813, -3.4382766746032827, 2.3862709016244743, 1.93046120531193, -2.3155783065309983, -10.658985248056416, -0.4077839794478119, 4.274625810233127, 0.8509057067876096, -1.5498146656741432, -11.379981538809073, 33.36956522786223, 7.965543005888329, -9.225795718168076, 6.287402424243714, 0.43117914139784735, 32.49990840279786, 7.958254528919758, 5.888089224309325, 2.8670798916349476, -3.1517774315239357, -2.199462063644175, -0.32975902185960504, 25.525875460191873, -14.613127052539584]\n",
      "P Values: [0.0, 0.0, 0.0009148658001134624, 0.1474978722777487, 1.307185883092643e-06, 1.1435297153639112e-13, 2.2715510508142245e-06, 0.38449332784245227, 0.17045724848154897, 4.720338820618508e-05, 0.006992399440711283, 0.0, 0.03633186642725539, 0.8335042211487846, 0.013249955285776993, 0.35946790803953244, 0.5516655206542067, 0.3924432706319543, 0.5981661295603415, 0.4146871349648551, 0.25516737382060195, 0.10265001361297266, 2.220446049250313e-16, 0.02170631801849443, 0.0, 0.0, 0.0, 2.2498500428236312e-05, 0.0, 0.02687632559499087, 0.000858662371039598, 0.0, 0.04437200661772822, 8.763279805168445e-05, 7.619651998247434e-08, 0.7274734807075336, 0.9785061461379163, 0.7002957068466378, 0.43095114564434467, 0.5514357032383728, 0.2587358036298173, 0.439215745713438, 0.4375363630298934, 0.4175933395813938, 0.5599839586748456, 0.7553471592219008, 0.8502834000240727, 0.9336236463192795, 0.0, 1.9984014443252818e-15, 5.434190786246518e-07, 0.0, 0.0, 0.008123716085488786, 0.04034258042351757, 0.033909111613856346, 0.020741821937319438, 0.65148092584236, 0.7635259581897071, 0.378470217516357, 0.3224894119472803, 0.789128640749722, 0.3289327272525706, 0.9332019637554221, 0.008798986147732002, 0.1056455639837175, 0.00017477524507802933, 0.11861934874560509, 0.08300190297144194, 0.1791721018255099, 1.539657290550167e-12, 0.0, 0.0, 0.20340988880270094, 0.0, 0.0002985348081787631, 2.0294294911238353e-08, 1.0389467064442215e-12, 0.0, 0.0, 0.0, 0.0, 2.3854261734790327e-06, 0.0894210996526521, 2.1401991290304068e-11, 1.8758888168823873e-05, 0.0, 0.0, 0.7743906528368947, 0.5400065676900803, 0.12461400829853186, 9.520278632257906e-05, 0.0837733691131266, 0.009319372588100894, 0.0020786200517080022, 0.030995498829967216, 3.820299632195656e-11, 0.0002814811460174482, 1.6982566950485278e-06, 1.7985612998927536e-14, 0.00021565834010406348, 0.5014041054826779, 0.3126286245572758, 0.0, 0.0, 2.351339155381993e-05, 0.020995038963818047, 0.0005856101522485613, 0.017021611773086143, 0.05355186173580684, 0.020582832718238242, 0.0, 0.6834329391386647, 1.9159297433679612e-05, 0.3948232814493924, 0.12118840868999503, 0.0, 0.0, 1.5543122344752192e-15, 0.0, 3.2382874159964103e-10, 0.6663388249760374, 0.0, 1.7763568394002505e-15, 3.916293511707636e-09, 0.0041434410571949964, 0.0016231627152183403, 0.027846814971493217, 0.7415825772002254, 0.0, 0.0]\n",
      "Dispersion: 4381699.856922461\n",
      "Null Deviance: 1118425598343.2312\n",
      "Residual Degree Of Freedom Null: 131837\n",
      "Deviance: 577100543055.6865\n",
      "Residual Degree Of Freedom: 131707\n",
      "AIC: 2390464.557988627\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-1710.5264473287384|\n",
      "|-1216.5909421300798|\n",
      "|-1662.7285439668353|\n",
      "|   -2782.7818109601|\n",
      "|  585.1467201378873|\n",
      "|-1469.6159901622805|\n",
      "|-1174.2892712486105|\n",
      "| -946.6829081912911|\n",
      "|-1128.4125779100132|\n",
      "| -2077.769240326788|\n",
      "|-231.09907911724895|\n",
      "|  -4399.98977757604|\n",
      "| -7535.429432539562|\n",
      "|-1085.3316115398352|\n",
      "| 211.08347315659512|\n",
      "| 324.23433662443585|\n",
      "| -679.6465361854257|\n",
      "| -913.2441826994645|\n",
      "| -572.7043474851416|\n",
      "| -3204.009322390421|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-------------------+--------------------+\n",
      "|         prediction|              label|            features|\n",
      "+-------------------+-------------------+--------------------+\n",
      "| -673.4007831340732|-3027.3376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "| -1047.002527582967| -3017.227684877636|[1.0,0.0,0.0,0.0,...|\n",
      "| -568.3047963250156|-3001.3376854879875|[0.0,0.0,0.0,1.0,...|\n",
      "|-2046.9471631239048|-2990.0576867086907|[1.0,0.0,0.0,0.0,...|\n",
      "|-1307.6560041619568|-2982.9976853353996|[0.0,0.0,0.0,0.0,...|\n",
      "| -82.70988015404964|  -2980.04768457246|[0.0,0.0,0.0,0.0,...|\n",
      "| -882.4693020468067|-2960.9876870138664|[1.0,0.0,0.0,0.0,...|\n",
      "|  2844.453179497631|-2957.3376854879875|[0.0,0.0,0.0,1.0,...|\n",
      "| 2781.4085156929123|-2957.3376854879875|[0.0,1.0,0.0,1.0,...|\n",
      "| 463.99299936060106|-2957.3376854879875|[0.0,1.0,0.0,1.0,...|\n",
      "| -3046.030148174707| -2949.757683656933|[1.0,0.0,0.0,1.0,...|\n",
      "|-2606.2242512118373|-2941.3376854879875|[0.0,1.0,0.0,0.0,...|\n",
      "|-1236.9247478506036|-2925.3376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "| -724.6458582542446|-2922.3376854879875|[1.0,0.0,0.0,1.0,...|\n",
      "| -1968.548089952853|-2918.2676857931633|[0.0,0.0,0.0,0.0,...|\n",
      "|-1023.5410864017895|-2915.1376885397453|[0.0,1.0,0.0,1.0,...|\n",
      "|  812.7859103430956| -2914.227684877636|[0.0,0.0,0.0,0.0,...|\n",
      "| -2129.098481942826| -2912.067688844921|[0.0,0.0,0.0,0.0,...|\n",
      "|-2227.0791122872056|-2910.9876870138664|[1.0,0.0,0.0,1.0,...|\n",
      "| -2259.303019421047|-2907.3376854879875|[0.0,0.0,1.0,0.0,...|\n",
      "|-2359.0662027300596|-2898.8876885397453|[0.0,0.0,0.0,0.0,...|\n",
      "| -903.9292986635273|   -2887.9676903708|[0.0,0.0,0.0,0.0,...|\n",
      "| -2853.937340612528| -2883.507683656933|[1.0,0.0,0.0,0.0,...|\n",
      "| -1246.406469748254| -2879.917687319042|[0.0,0.0,0.0,1.0,...|\n",
      "| -2559.980862919787|-2877.3376854879875|[1.0,0.0,0.0,1.0,...|\n",
      "|-2393.7701258728225| -2872.987679384472|[1.0,0.0,0.0,0.0,...|\n",
      "|-2145.3178507817024| -2864.207680605175|[0.0,0.0,0.0,1.0,...|\n",
      "|-1543.5443068258205|-2862.3376854879875|[0.0,1.0,0.0,0.0,...|\n",
      "|-2072.8583859199716|-2862.1376885397453|[1.0,0.0,0.0,1.0,...|\n",
      "|  2553.815437946101|-2857.3376854879875|[0.0,0.0,1.0,0.0,...|\n",
      "| -984.0654485459199|-2855.0976799948235|[1.0,1.0,0.0,0.0,...|\n",
      "| -2542.416785271684|-2852.6576928122063|[1.0,0.0,0.0,0.0,...|\n",
      "| -3006.160401674432| -2851.957680605175|[1.0,0.0,0.0,1.0,...|\n",
      "| -2240.629682439415| -2850.187691591503|[0.0,0.0,0.0,1.0,...|\n",
      "|-1354.5927253180366|-2845.5476922018547|[0.0,0.0,0.0,0.0,...|\n",
      "| -974.3784933595098| -2844.777687929394|[1.0,0.0,0.0,1.0,...|\n",
      "|-3567.8522028037805|-2843.1176842672844|[1.0,0.0,0.0,0.0,...|\n",
      "| -748.9801652346723|-2842.1576928122063|[0.0,0.0,0.0,1.0,...|\n",
      "|-2069.2673419380394| -2839.477684877636|[0.0,0.0,0.0,0.0,...|\n",
      "| -3358.317673332981|   -2839.4676903708|[0.0,0.0,0.0,0.0,...|\n",
      "|-2061.3415210342478| -2837.257683656933|[1.0,1.0,0.0,0.0,...|\n",
      "|-1542.2804513451306| -2836.987679384472|[0.0,0.0,0.0,0.0,...|\n",
      "| -2319.528230931146| -2835.737679384472|[1.0,0.0,0.0,0.0,...|\n",
      "|-1012.1391047864308|-2833.8576897604485|[0.0,0.0,0.0,1.0,...|\n",
      "|-1016.1248980590317|-2823.7876824362297|[0.0,0.0,0.0,0.0,...|\n",
      "| -2658.379984074026|-2821.8176812155266|[0.0,0.0,0.0,1.0,...|\n",
      "| -1688.021873424841|-2808.8176812155266|[0.0,1.0,0.0,0.0,...|\n",
      "|-1995.3269913589434|-2803.8376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "|-1697.3390101415075|-2801.5476922018547|[0.0,0.0,0.0,0.0,...|\n",
      "|-1447.5456961060318| -2800.917687319042|[1.0,1.0,0.0,0.0,...|\n",
      "|-1673.3733702705954| -2800.007683656933|[0.0,0.0,0.0,0.0,...|\n",
      "|-1492.7383693720953|-2794.8676842672844|[1.0,1.0,0.0,0.0,...|\n",
      "|-2226.3491231325734|-2792.6476830465813|[0.0,0.0,0.0,0.0,...|\n",
      "|-1420.8441366617149| -2792.517678163769|[0.0,0.0,0.0,0.0,...|\n",
      "|-1188.6551878674495|-2791.0976799948235|[0.0,1.0,0.0,0.0,...|\n",
      "| -2730.443531017039|-2789.6576928122063|[0.0,0.0,0.0,0.0,...|\n",
      "|-2162.1955268153183|-2788.5876854879875|[0.0,0.0,0.0,0.0,...|\n",
      "| -2378.744520524009| -2785.267678163769|[0.0,0.0,0.0,0.0,...|\n",
      "|-1224.9297764797088| -2781.247689150097|[0.0,0.0,0.0,1.0,...|\n",
      "|-3242.5684135700903| -2779.037697695019|[1.0,0.0,0.0,1.0,...|\n",
      "| -547.5327415752981|-2775.3276757223625|[0.0,0.0,0.0,1.0,...|\n",
      "| -795.7013139695828|-2773.3976830465813|[0.0,0.0,0.0,0.0,...|\n",
      "| -739.5479886233061|-2773.2976769430657|[0.0,0.0,0.0,0.0,...|\n",
      "| -938.6561255669967|  -2773.19767083955|[0.0,0.0,0.0,0.0,...|\n",
      "|-1717.1100657498491|-2773.1276940329094|[1.0,1.0,0.0,0.0,...|\n",
      "| -1246.645983521239|-2770.7976769430657|[0.0,0.0,0.0,0.0,...|\n",
      "| -2094.599599136862|-2770.3776940329094|[0.0,0.0,0.0,0.0,...|\n",
      "|-3427.0705892878987| -2768.787697695019|[0.0,0.0,0.0,0.0,...|\n",
      "|-2123.6325581933042| -2768.417672060253|[0.0,0.0,0.0,0.0,...|\n",
      "| -1600.981584037766| -2765.437691591503|[0.0,0.0,0.0,0.0,...|\n",
      "| -818.0408306982436|  -2764.69767083955|[1.0,0.0,0.0,0.0,...|\n",
      "| -2711.268015205429| -2763.037697695019|[1.0,0.0,0.0,1.0,...|\n",
      "| -655.1478773003628|-2762.8776940329094|[0.0,0.0,0.0,0.0,...|\n",
      "|-1919.9085035678877| -2762.787697695019|[0.0,0.0,0.0,0.0,...|\n",
      "| -1320.228910430354|-2761.0676964743157|[0.0,0.0,0.0,1.0,...|\n",
      "|  -949.958089298997|-2759.4076928122063|[1.0,0.0,0.0,0.0,...|\n",
      "| -2178.168589252302|-2759.1376732809563|[0.0,0.0,0.0,0.0,...|\n",
      "|  606.1284547581108|-2758.8476952536125|[0.0,1.0,0.0,0.0,...|\n",
      "| -1323.358092843101| -2757.937691591503|[0.0,1.0,0.0,0.0,...|\n",
      "| 3495.6213147325043|-2757.3376854879875|[0.0,0.0,0.0,0.0,...|\n",
      "| 2377.4147209669773|-2757.3376854879875|[0.0,1.0,1.0,0.0,...|\n",
      "|-2885.6917706197805| -2755.167672060253|[1.0,0.0,0.0,0.0,...|\n",
      "|   9416.95044506848|   -2753.9676903708|[0.0,1.0,1.0,1.0,...|\n",
      "| -840.4722949816069|-2752.8276757223625|[0.0,0.0,1.0,0.0,...|\n",
      "|-1755.6012921735564| -2752.707680605175|[1.0,1.0,0.0,0.0,...|\n",
      "|-1124.4335215219617| -2752.237679384472|[1.0,0.0,0.0,1.0,...|\n",
      "|-2977.2673995287905|-2751.6576928122063|[1.0,1.0,0.0,0.0,...|\n",
      "|-1786.0195575211155| -2751.517678163769|[0.0,0.0,0.0,1.0,...|\n",
      "|-2443.7710008823915| -2750.737679384472|[0.0,0.0,0.0,0.0,...|\n",
      "|-1338.5601490013887|-2750.3976830465813|[0.0,0.0,0.0,0.0,...|\n",
      "| -734.2914685310734| -2745.927681825878|[0.0,0.0,0.0,0.0,...|\n",
      "|-1966.1123422090745|-2745.8376854879875|[1.0,0.0,0.0,0.0,...|\n",
      "|-1981.2899225834644| -2745.537697695019|[0.0,0.0,0.0,0.0,...|\n",
      "| -2338.462407028319|-2744.5976952536125|[1.0,0.0,0.0,0.0,...|\n",
      "| -2358.449585289518|-2743.8376854879875|[1.0,0.0,0.0,1.0,...|\n",
      "|-2014.3549016167694| -2743.517678163769|[0.0,0.0,0.0,0.0,...|\n",
      "|-2149.5045940057685|-2742.1476830465813|[1.0,0.0,0.0,0.0,...|\n",
      "| -505.2075447355519|-2741.8976830465813|[1.0,1.0,0.0,0.0,...|\n",
      "|-2450.6697624237922| -2741.777687929394|[1.0,0.0,0.0,1.0,...|\n",
      "| -2203.294898934089|-2741.1576928122063|[1.0,0.0,0.0,0.0,...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 1335.43\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Load training data\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(df_Perceptron_train)\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(100)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|         features|prediction|\n",
      "+-----------------+----------+\n",
      "|[1.0,0.0,0.0,0.0]|       1.0|\n",
      "|[1.0,1.0,1.0,1.0]|       0.0|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([\n",
    "(0.0, Vectors.dense([0.0, 0.0,0.0, 0.0])),\n",
    "(1.0, Vectors.dense([0.0, 1.0,1.0, 1.0])),\n",
    "(1.0, Vectors.dense([1.0, 0.0,0.0, 0.0])),\n",
    "(0.0, Vectors.dense([1.0, 1.0,1.0, 1.0]))], [\"label\", \"features\"])\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[4, 2, 4], blockSize=1, seed=123)\n",
    "model = mlp.fit(df)\n",
    "\n",
    "testDF = spark.createDataFrame([\n",
    "(Vectors.dense([1.0, 0.0,0.0, 0.0]),),\n",
    "(Vectors.dense([1.0, 1.0,1.0, 1.0]),)], [\"features\"])\n",
    "model.transform(testDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
