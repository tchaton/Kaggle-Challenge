{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import and install of spark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "5\n",
      "defaultdict(<class 'int'>, {'akka': 1, 'hadoop': 1, 'scala': 1, 'java': 1, 'spark': 1})\n"
     ]
    }
   ],
   "source": [
    "#### This cell is to make spark work on a windows laptop\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\spark-2.0.1-bin-hadoop2.7\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\")\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\")\n",
    "#os.environ['SPARK_EXECUTOR_MEMORY']=\"5G\"\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SQLContext\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext('local[*]')\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\"])\n",
    "print (words.count())\n",
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import data and take off header</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before cleaning header: 188319\n",
      "number of rows without header: 188318\n",
      "Number of partitions :2\n"
     ]
    }
   ],
   "source": [
    "input_path = 'train.csv'\n",
    "raw_data = sc.textFile(input_path)\n",
    "\n",
    "print(\"number of rows before cleaning header:\",raw_data.count())\n",
    "\n",
    "header = raw_data.first()\n",
    "\n",
    "cleaned_data = raw_data.filter(lambda row : row != header)\n",
    "\n",
    "print(\"number of rows without header:\",cleaned_data.count())\n",
    "print('Number of partitions :'+str(cleaned_data.getNumPartitions()))\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'loss']\n",
      "Length of names 132\n"
     ]
    }
   ],
   "source": [
    "names = header.split(';')[0].split(',')\n",
    "print(names)\n",
    "print(\"Length of names\",len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = names[1:117]\n",
    "conts = names[117:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_StructField(string):\n",
    "    hint = string[:3]\n",
    "    if hint == \"cat\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"con\":\n",
    "        datatype = types.FloatType()\n",
    "    elif hint == \"id\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"los\":\n",
    "        datatype = types.FloatType()\n",
    "    else:\n",
    "        raise ValueError(\"Can\\'t read this string:\" + hint )\n",
    "\n",
    "    return types.StructField(string, datatype, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list = [create_StructField(string) for string in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_schema = types.StructType(structField_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryeval(val,column_number):\n",
    "    if column_number == 0:\n",
    "        return int(val)\n",
    "    elif 1 <= column_number <= 116:\n",
    "        return val\n",
    "    elif 117 <= column_number <= 131:\n",
    "        return float(val)\n",
    "    else:\n",
    "        raise Exception(\"There is a big problem\")\n",
    "\n",
    "def to_tuple(row):\n",
    "    list_strings = row.split(',')\n",
    "    return tuple(tryeval(val, n) for n, val in enumerate(list_strings))\n",
    "\n",
    "cleaned_data_splitted = cleaned_data.map(lambda x:to_tuple(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tuples(list_):\n",
    "    return tuple((string,) for string in list_)\n",
    "\n",
    "def fusion(x, y):\n",
    "    return tuple(tuple(set(xi + yi)) for xi, yi in zip(x,y))\n",
    "\n",
    "list_of_dictionaries = []\n",
    "a = cleaned_data_splitted.map(lambda x: to_tuples(x[1:117]))\n",
    "a = a.reduce(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tuples = tuple(tuple(sorted(tup)) for tup in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tup in sorted_tuples:\n",
    "    my_dict = dict()\n",
    "    for idx, cat in enumerate(tup):\n",
    "        my_dict[cat] = idx\n",
    "    list_of_dictionaries.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bListOfDictionaries = sc.broadcast(list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(row):\n",
    "    strings = row[1:117]\n",
    "    my_dicts = bListOfDictionaries.value\n",
    "    tuple_of_ints = ()\n",
    "    for dict_, string in zip(my_dicts, strings):\n",
    "        try:\n",
    "            tuple_of_ints += (dict_[string],)\n",
    "        except KeyError:\n",
    "            tuple_of_ints += (0.0,)\n",
    "    return (row[0],) + tuple_of_ints + row[117:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_rdd = cleaned_data_splitted.map(lambda row:replace(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd, schema = data_schema).coalesce(12).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>PART OF MAKING A PREDICTION </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_index(tup):\n",
    "    result = ()\n",
    "    for idx in list_indices:\n",
    "        result += (tup[idx-1],)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd, schema = data_schema).coalesce(12).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "last = len(names)\n",
    "list_indices = [i for i in range(2,last)]\n",
    "L = df.rdd.count()\n",
    "print(last)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list_selected = [create_StructField(string) for i,string in enumerate(names) if i in list_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df.rdd.map(lambda x: (x[-1])).mean()\n",
    "stdev = df.rdd.map(lambda x: (x[-1])).stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def transform_loss(x):\n",
    "    return int(x-mean)\n",
    "\n",
    "df_Perceptron = sqlContext.createDataFrame(df.rdd.map(lambda x: (transform_loss(x[-1]), Vectors.dense(keep_index(x)))), [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "| -824|\n",
      "|-1753|\n",
      "|  -32|\n",
      "|-2097|\n",
      "| -273|\n",
      "| 2105|\n",
      "|-1905|\n",
      "|  548|\n",
      "| 7242|\n",
      "| 3147|\n",
      "| 3359|\n",
      "| 2928|\n",
      "|-1844|\n",
      "|-1965|\n",
      "|-2452|\n",
      "|-1641|\n",
      "| 3571|\n",
      "| -378|\n",
      "| 1129|\n",
      "|  760|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Perceptron.select(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0, 0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Perceptron.select(\"features\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DecisionTreeRegressor</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "| -870.4509177027827|-3016|[0.0,0.0,0.0,0.0,...|\n",
      "|   239.360745949251|-3000|[0.0,0.0,0.0,1.0,...|\n",
      "|-1512.5543143664117|-2990|[1.0,0.0,0.0,0.0,...|\n",
      "|-1512.5543143664117|-2982|[0.0,0.0,0.0,0.0,...|\n",
      "|   239.360745949251|-2980|[0.0,0.0,0.0,0.0,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2319.11\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_48628d8fca3c94c39325) of depth 5 with 61 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32).fit(df_Perceptron)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(df_Perceptron_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GeneralizedLinearRegression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|-1384.3056890041448|-3028|[0.0,0.0,0.0,0.0,...|\n",
      "|-1791.2807233646777|-3016|[0.0,0.0,0.0,0.0,...|\n",
      "| 145.82262524537418|-2980|[0.0,0.0,0.0,0.0,...|\n",
      "|-2083.7987068297894|-2975|[0.0,0.0,0.0,1.0,...|\n",
      "|-1865.7978519117746|-2971|[1.0,0.0,0.0,0.0,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2046.89\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Load training data\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(df_Perceptron_train)\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RandomForestRegressor</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|-1020.4532384166043|-3028|[0.0,0.0,0.0,0.0,...|\n",
      "|-1021.3137218427297|-3017|[1.0,0.0,0.0,0.0,...|\n",
      "|-1020.4532384166043|-2982|[0.0,0.0,0.0,0.0,...|\n",
      "| 2554.0547511405034|-2957|[0.0,0.0,0.0,1.0,...|\n",
      "|-1253.5649723904537|-2937|[1.0,0.0,0.0,1.0,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2161.53\n",
      "RandomForestRegressionModel (uid=rfr_2762b2dc24dc) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_Perceptron)\n",
    "\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(df_Perceptron_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient-boosted tree regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|-1340.1336595367884|-3016|[0.0,0.0,0.0,0.0,...|\n",
      "|-1969.8272770670592|-2998|[1.0,0.0,0.0,0.0,...|\n",
      "|-400.04707278890606|-2980|[0.0,0.0,0.0,0.0,...|\n",
      "|-1405.6426707753062|-2976|[0.0,0.0,0.0,0.0,...|\n",
      "|-1799.0712097941314|-2971|[1.0,0.0,0.0,0.0,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2165.9\n",
      "GBTRegressionModel (uid=GBTRegressor_4769b99b79932a236350) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "Fre\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_Perceptron)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(df_Perceptron_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IsotonicRegression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression, IsotonicRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Trains an isotonic regression model.\n",
    "model = IsotonicRegression().fit(df_Perceptron_train)\n",
    "# Makes predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>One-vs-Rest classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(df_Perceptron_train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(df_Perceptron_test)\n",
    "\n",
    "# obtain evaluator.\n",
    "rmse = MulticlassClassificationEvaluator(metricName=\"mae\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error : \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df_Perceptron_train)\n",
    "result = model.transform(df_Perceptron_test).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)\n",
    "# $example off$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision tree classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df_Perceptron)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_Perceptron)\n",
    "\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(df_Perceptron_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_Perceptron_test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (rmse))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LogisticRegression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(df_Perceptron_train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(df_Perceptron_test)\n",
    "\n",
    "# obtain evaluator.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"rmse\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Test Error : \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NaiveBayes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "(df_Perceptron_train,df_Perceptron_test) = df_Perceptron.randomSplit([0.7, 0.3])\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(df_Perceptron_train)\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(df_Perceptron_test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"rmse\")\n",
    "print(\"rmse: \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
